---
title: "474_random_forest"
author: "Chloe Florence"
date: "2025-11-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing Data

```{r, warning=FALSE}
library(dplyr)
library(class)
library(leaps)
library(tree)
library(caret)
library(randomForest)
# creating binomial predictor for type of wine
red_wine <- read.csv("winequality-red.csv", header = TRUE, sep = ";")
red_wine["type"] = 1
white_wine <- read.csv("winequality-white.csv", header = TRUE, sep = ";")
white_wine["type"] = 0
# creating binomial predictor for if a wine is good or not.
wine <- bind_rows(red_wine, white_wine)
wine$good <- ifelse(wine$quality > 6, 1, 0)
```

## Trees

First creating one individual tree

```{r}
wine$good <- factor(ifelse(wine$quality > 6, "Yes", "No"))
set.seed(1)
train <- sample(6497, 5198)
wine.test <- wine[-train, ]
wine.good <- wine[-train, 14]

# control here forces more leaves, otherwise it classifies everything to bad with ~ 19% error
tree.wine <- tree(good ~ . -quality, data = wine, subset = train, control = tree.control(nobs = length(train), mincut = 1, mindev = 0.001))

tree.pred <- predict(tree.wine, wine.test, type = "class")

table(tree.pred, wine.good)

mean(tree.pred != wine.good)
```
16.6% classification rate for just creating one tree. 

```{r}
plot(tree.wine)
#text(tree.wine, pretty = 0)
```

### Cross Validation Trees

dev - number of cross validation errors

k - is the alpha tuning value

```{r}
set.seed(1)
cv.wine <- cv.tree(tree.wine, FUN = prune.misclass)
#cv.wine
#par(mfrow = c(1,2))
#plot(cv.wine$size, cv.wine$dev, type = "b")
#plot(cv.wine$k, cv.wine$dev, type = "b")
```

## Bagged regression

### cross validation for the best value of ntree

```{r}
ctrl <- trainControl(method = "cv", number = 5)

ntree_values <- seq(25, 1000, by = 25)

cv.bag.model <- lapply(ntree_values, function(nt){
  train(good ~ . - quality, data = wine, subset = train, 
        method = "rf", trControl = ctrl, tuneLength = 1, 
        ntree = nt)
}) 

names(cv.bag.model) <- paste0("ntree_", ntree_values)

results <- sapply(cv.bag.model, function(m) max(m$results$Accuracy))
```

```{r}
plot(ntree_values, results, ylab = "Prediction Accuracy", xlab = "ntree value")
abline(v = ntree_values[which.max(results)], col = "blue", lty = 2)
points(ntree_values[which.max(results)],max(results), col = "blue")
text(ntree_values[which.max(results)],max(results), label = "(625,0.8834)", pos = 4 )
```


```{r}
set.seed(1)
bag.wine <- randomForest(good ~ . - quality, data = wine, subset = train, mtry = 12, ntree = 625)
bag.wine
```

```{r}
yhat.bag <- predict(bag.wine, newdata = wine[-train, ])
table(yhat.bag, wine.good)
mean(yhat.bag!= wine.good)
```

test error of 10.39% with bagging.

## Random forest

### Cross validation for best mtry value

```{r, warning = FALSE}
ctrl <- trainControl(method = "cv", number = 5)

cv.model <- train(good ~ . - quality, data = wine, subset = train, method = "rf", trControl = ctrl, tunrGrid = expand.grid(mtry = c(1:12)))

cv.model
```




```{r}
set.seed(1)
rf.wine <- randomForest(good ~ . - quality, data = wine, subset = train, mtry = 2, importance = TRUE)
rf.wine
yhat.rf <- predict(rf.wine, newdata = wine[-train, ])
table(yhat.rf, wine.good)
mean(yhat.rf != wine.good)
```

This has a test error of 9.62% the lowest yet. 

```{r}
importance(rf.wine)
```

```{r}
varImpPlot(rf.wine)
```



